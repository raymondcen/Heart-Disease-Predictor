Logistic Regression:
  penalty: ["l2"]
  C: [0.01, 0.1, 1, 10]
  solver: ["saga"]
  class_weight: ["balanced"]

Random Forest:
  n_estimators: [100, 300]
  max_depth: [10, 20, 30, None]
  min_samples_split: [2, 5, 10]
  min_samples_leaf: [1, 2, 4]
  max_features: ["sqrt", "log2"]
  bootstrap: [True]
  class_weight: ["balanced"]

Decision Tree:
  max_depth: [10, 20, 30, None]
  min_samples_split: [2, 10, 20]
  min_samples_leaf: [1, 2, 4, 10]
  max_features: ["sqrt", "log2", None]
  class_weight: ["balanced"]

XGBoost:
  n_estimators: [100, 300]
  max_depth: [3, 5, 7]
  learning_rate: [0.01, 0.05, 0.1]
  subsample: [0.6, 0.8, 1.0]
  colsample_bytree: [0.6, 0.8, 1.0]
  scale_pos_weight: [16.709263895843765] 

CatBoost:
  iterations: [500, 1000]
  depth: [4, 6, 8]
  learning_rate: [0.01, 0.05, 0.1]
  l2_leaf_reg: [3, 5, 7]
  class_weights: [[1,10]] 
  auto_class_weights: ['Balanced']
  
AdaBoost:
  n_estimators: [50, 100, 200]
  learning_rate: [0.01, 0.05, 0.1, 1]

GradientBoosting:
  n_estimators: [100, 200]
  learning_rate: [0.1, 0.05, 0.01, 1]
  max_depth: [3, 5]
  subsample: [0.8]
  min_samples_leaf: [2, 4]

K-Nearest Neighbors:
  n_neighbors: [5, 7, 9]
  weights: ["distance", "uniform"]  # distance-weighted helps minority
  metric: ["minkowski", "euclidean"]
