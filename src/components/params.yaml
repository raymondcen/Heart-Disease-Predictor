Logistic Regression:
  penalty: ["l2"]
  C: [0.05, 0.08, 0.1, 0.12, 0.15]
  solver: ["saga"]
  class_weight: ["balanced"]

Random Forest:
  n_estimators: [40, 50, 60, 75]
  max_depth: [35,40,45,None]
  min_samples_split: [4,5,6]
  min_samples_leaf: [ 3, 4,5,6]
  max_features: ["sqrt", "log2"]
  bootstrap: [True]
  class_weight: ["balanced"]

# Decision Tree:
#   max_depth: [10, 20, 30, None]
#   min_samples_split: [2, 10, 20]
#   min_samples_leaf: [1, 2, 4, 10]
#   max_features: ["sqrt", "log2", None]
#   class_weight: ["balanced"]

XGBoost:
  n_estimators: [400, 500, 600, 700, 800]
  max_depth: [4, 5, 6]
  learning_rate: [0.09, 0.1, 0.12, 0.14]
  subsample: [0.2, 0.4, 0.6, 0.8, 1.0]
  colsample_bytree: [0.7, 0.8, 0.9, 1.0]
  # scale_pos_weight: [16.709263895843765] 
  tree_method: ["hist"]

CatBoost:
  iterations: [1000, 1500, 2000, 2500, 3000]
  depth: [6, 8, 10, 12]
  learning_rate: [0.06, 0.8, 0.1, 0.12]
  l2_leaf_reg: [5, 7, 8, 9, 10]
  auto_class_weights: ['Balanced']
  
# AdaBoost:
#   n_estimators: [75, 100, 125]
#   learning_rate: [0.1, 0.8, 1, 1.2]

GradientBoosting:
  n_estimators: [65, 70, 75, 90, 100, 110]
  learning_rate: [0.05, 0.01, 0.08, 0.1, 0.12]
  max_depth: [1, 2, 3, 5]
  subsample: [0.6, 0.7, 0.8, 0.9,1]
  min_samples_leaf: [3, 4, 5, 6]

# K-Nearest Neighbors:
#   n_neighbors: [5, 7, 9]
#   weights: ["distance", "uniform"]  # distance-weighted helps minority
#   metric: ["minkowski", "euclidean"]
