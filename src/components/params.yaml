Logistic Regression:
  penalty: ["l1", "l2"]
  C: [0.02, 0.03, 0.04, 0.1, 0.2, 0.3]
  solver: ["saga"]  
  # C: [0.2]
  # penalty: ['l2']
  # solver: ['saga']

Random Forest:
  n_estimators: [25,50, 75, 100,125,150]  
  max_depth: [15,25,30,35,40,45]  
  min_samples_split: [4,5,6,7,8]  
  min_samples_leaf: [1,3, 4, 5, 6, 8]  
  max_leaf_nodes: [50, 100, 150]
  max_features: ["sqrt"]  

XGBoost:
  n_estimators: [50,75, 100, 150, 200]
  max_depth: [3, 4, 5,6]             
  learning_rate: [0.05, 0.07, 0.1, 0.12] 
  subsample: [0.6, 0.7, 0.8,0.9]       
  colsample_bytree: [0.6, 0.7, 0.8,0.9]
  # min_child_weight: [3, 5, 7]      
  # gamma: [0, 0.1, 0.2]             
  # reg_alpha: [0, 0.01, 0.1]        
  # reg_lambda: [1, 1.5, 2]          

CatBoost:
  iterations: [100, 200, 300,400,500]
  depth: [4, 5, 6,7]
  learning_rate: [0.05, 0.07, 0.1, 0.12]
  l2_leaf_reg: [8,10,12,15, 20]
  # border_count: [32, 64, 128]
  # bagging_temperature: [0.5, 1.0]
  # random_strength: [0.5, 1.0, 2.0]
  
GradientBoosting:
  n_estimators: [75, 90, 100, 125]  
  learning_rate: [.07, 0.08, 0.09, 0.1]  
  max_depth: [1, 2, 3,4,5]  
  subsample: [0.6, 0.7, 0.8, 0.9]  
  min_samples_leaf: [3, 4, 5]  
