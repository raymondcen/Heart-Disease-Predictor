Logistic Regression:
  penalty: ["l2"]
  C: [0.05, 0.08, 0.1, 0.12, 0.15, 0.18, 0.2, 0.25, 0.3]  
  solver: ["saga", "liblinear"]  
  class_weight: ["balanced"]

Random Forest:
  n_estimators: [50, 75, 100, 125, 150]  
  max_depth: [25, 30, 35, 40, 45]  
  min_samples_split: [4, 5, 6, 7, 8]  
  min_samples_leaf: [3, 4, 5]  
  max_features: ["sqrt", "log2"]  
  bootstrap: [True]
  class_weight: ["balanced"]

# Decision Tree:
#   max_depth: [10, 20, 30, None]
#   min_samples_split: [2, 10, 20]
#   min_samples_leaf: [1, 2, 4, 10]
#   max_features: ["sqrt", "log2", None]
#   class_weight: ["balanced"]

XGBoost:
  n_estimators: [400, 500, 600, 700, 800]  
  max_depth: [4, 5, 6, 7]  
  learning_rate: [0.08, 0.09, 0.1, 0.11, 0.12]  
  subsample: [0.4, 0.5, 0.6, 0.7]  
  colsample_bytree: [0.7, 0.8, 0.9]  

CatBoost:
  iterations: [500, 750, 1000, 1250]  
  depth: [7, 8, 9, 10]  
  learning_rate: [0.08, 0.09, 0.1, 0.11]  
  l2_leaf_reg: [6, 7, 8, 9]  
  auto_class_weights: ['Balanced']
  
# AdaBoost:
#   n_estimators: [75, 100, 125]
#   learning_rate: [0.1, 0.8, 1, 1.2]

GradientBoosting:
  n_estimators: [75, 90, 100, 125]  
  learning_rate: [0.08, 0.09, 0.1, 0.11]  
  max_depth: [1, 2, 3]  
  subsample: [0.6, 0.7, 0.8, 0.9]  
  min_samples_leaf: [3, 4, 5]  

# K-Nearest Neighbors:
#   n_neighbors: [5, 7, 9]
#   weights: ["distance", "uniform"]  # distance-weighted helps minority
#   metric: ["minkowski", "euclidean"]
